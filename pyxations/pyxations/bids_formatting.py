import os
import shutil
import subprocess
import numpy as np
import pandas as pd



def dataset_to_bids(folder_path, files_folder_path, dataset_name, session_substrings=2):
    """
    Convert a dataset to BIDS format.

    Args:
        folder_path (str): Path to the folder where the BIDS dataset will be created.
        files_folder_path (str): Path to the folder containing the EDF files.
        The EDF files are assumed to have the ID of the subject at the beginning of the file name, separated by an underscore.
        dataset_name (str): Name of the BIDS dataset.
        session_substrings (int): Number of substrings to use for the session ID. Default is 2.

    Returns:
        None
    """

    # List all files in the folder
    files = os.listdir(files_folder_path)

    # Create a new folder for the BIDS dataset
    bids_folder_path = os.path.join(folder_path, dataset_name)
    os.makedirs(bids_folder_path, exist_ok=True)

    # Create subfolders for each subject
    subject_folders = []
    for file in files:
        if file.lower().endswith(".edf"):
            subject_id = file.split("_")[0]
            subject_folder_path = os.path.join(bids_folder_path, "sub-" + subject_id)
            os.makedirs(subject_folder_path, exist_ok=True)
            subject_folders.append(subject_folder_path)

    # Create subfolders for each session for each subject
    for subject_folder in subject_folders:
        for file in files:
            file_lower = file.lower()
            subject_id = os.path.basename(subject_folder)[4:]
            session_id = "_".join(file.split("_")[1:session_substrings+1])
            if file_lower.endswith(".edf") and file.startswith(subject_id):
                move_file_to_bids_folder(os.path.join(files_folder_path, file), bids_folder_path, subject_id, session_id, 'ET')
            if file_lower.endswith(".bdf") and file.startswith(subject_id):
                move_file_to_bids_folder(os.path.join(files_folder_path, file), bids_folder_path, subject_id, session_id, 'EEG')
            if (file_lower.endswith(".log") or file_lower.endswith(".csv")) and file.startswith(subject_id):                
                move_file_to_bids_folder(os.path.join(files_folder_path, file), bids_folder_path, subject_id, session_id, 'Psycopy')
    return bids_folder_path

def move_file_to_bids_folder(file_path, bids_folder_path, subject_id, session_id,tag):
    session_folder_path = os.path.join(bids_folder_path, "sub-" + subject_id, "ses-" + session_id,tag)
    os.makedirs(session_folder_path, exist_ok=True)
    shutil.move(file_path, session_folder_path)
    

def convert_edf_to_ascii(edf_file_path, output_dir):
    """
    Convert an EDF file to ASCII format using edf2asc.

    Args:
        edf_file_path (str): Path to the input EDF file.
        output_dir (str): Directory to save the ASCII file. If None, the ASCII file will be saved in the same directory as the input EDF file.

    Returns:
        str: Path to the generated ASCII file.
    """
    # Check if edf2asc is installed
    if not shutil.which("edf2asc"):
        raise FileNotFoundError("edf2asc not found. Please make sure EyeLink software is installed and accessible in the system PATH.")

    # Set output directory
    if output_dir is None:
        raise ValueError("Output directory must be specified.")

    # Generate output file path
    edf_file_name = os.path.basename(edf_file_path)
    ascii_file_name = os.path.splitext(edf_file_name)[0] + ".asc"
    ascii_file_path = os.path.join(output_dir, ascii_file_name)

    # Run edf2asc command with the -f flag, only run it if the file does not already exist
    if not os.path.exists(ascii_file_path):
        subprocess.run(["edf2asc", "-f", edf_file_path, ascii_file_path])

    return ascii_file_path



def parse_edf_eyelink(edf_file_path, msg_keywords,derivatives_folder,keep_ascii=True):
    """
    Parse an EDF file generated by EyeLink system.

    Args:
        edf_file_path (str): Path to the input EDF file.
        msg_keywords (list of str): List of strings representing keywords to filter MSG lines.

    Returns:
        tuple: A tuple containing five pandas DataFrames:
            - Header information DataFrame
            - MSG lines DataFrame filtered by msg_keywords
            - Calibration information DataFrame
            - EyeLink events DataFrame
            - Raw sample data DataFrame
    """
    # Convert EDF to ASCII
    ascii_file_path = convert_edf_to_ascii(edf_file_path,derivatives_folder)

    # ===== READ IN FILES ===== #
    # Read in EyeLink file
    
    f = open(ascii_file_path,'r')
    fileTxt0 = f.read().splitlines(True) # split into lines
    fileTxt0 = np.array(fileTxt0) # concert to np array for simpler indexing
    f.close()


    # Separate lines into samples and messages
    print('Sorting lines...')
    nLines = len(fileTxt0)
    lineType = np.array(['OTHER']*nLines,dtype='object')


    # Usar lo de mne, particularmente para calibration.
    # En sample tendría que filtrar lo que viene después de START y antes de END.
    

    calibration_flag = False
    start_flag = False
    for iLine in range(nLines):
        if len(fileTxt0[iLine])<2:
            lineType[iLine] = 'EMPTY'
        elif fileTxt0[iLine].startswith('*'):
            lineType[iLine] = 'HEADER'
        # If there is a !CAL in the line, it is a calibration line
        elif '!CAL' in fileTxt0[iLine]:
            lineType[iLine] = 'Calibration'
            calibration_flag = True
        elif fileTxt0[iLine].split()[0] == 'START' and calibration_flag:
            calibration_flag = False
            start_flag = True
        elif calibration_flag:
            lineType[iLine] = 'Calibration'
        elif not start_flag: # Data before the first successful calibration is discarded. 
            # After the first successul calibration, EVERY sample is taken into account.
            lineType[iLine] = 'Non_calibrated_samples'
        elif fileTxt0[iLine].split()[0] == 'MSG' and any(keyword in fileTxt0[iLine] for keyword in msg_keywords):
            lineType[iLine] = 'MSG'
        elif fileTxt0[iLine].split()[0] == 'ESACC':
            lineType[iLine] = 'ESACC'
        elif fileTxt0[iLine].split()[0] == 'EFIX':
            lineType[iLine] = 'EFIX'
        elif fileTxt0[iLine].split()[0] == 'EBLINK':
            lineType[iLine] = 'EBLINK'
        elif fileTxt0[iLine].split()[0][0].isdigit() or fileTxt0[iLine].split()[0].startswith('-'):
            lineType[iLine] = 'SAMPLE'
        else:
            lineType[iLine] = 'OTHER'
        
 
    # ===== PARSE EYELINK FILE ===== #
    # Import Header
    print('Parsing header...')
    dfHeader = pd.read_csv(ascii_file_path,skiprows=np.nonzero(lineType!='HEADER')[0],header=None,sep='\s+')
    # Merge columns into single strings
    dfHeader = dfHeader.apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1)


    # Import Calibration
    print('Parsing calibration...')
    iCal = np.nonzero(lineType!='Calibration')[0]
    dfCalib = pd.read_csv(ascii_file_path,skiprows=iCal,names=np.arange(9))
    # Get the first column (which is of type string) and see which row contains the string 'GAZE_COORDS'
    i_gaze_coords = dfCalib[0].str.contains('GAZE_COORDS')
    # Get the first row that matches the condition
    i_gaze_coords = i_gaze_coords[i_gaze_coords].index[0]
    # Grab the value of that row in the first column, turn it into a string and split it by spaces, then get the 5th and 6th elements, which are the screen resolution
    screen_res = dfCalib.iloc[i_gaze_coords][0].split()[5:7]
    dfHeader.loc[len(dfHeader.index)] = "** SCREEN SIZE: "+ " ".join(screen_res)



    # Import Message
    print('Parsing messages...')
    i_msg = np.nonzero(lineType == 'MSG')[0]
    t_msg = []
    txt_msg = []
    for i in range(len(i_msg)):
        # separate MSG prefix and timestamp from rest of message
        info = fileTxt0[i_msg[i]].split()
        # extract info
        t_msg.append(int(info[1]))
        txt_msg.append(' '.join(info[2:]))
    dfMsg = pd.DataFrame({'time': t_msg, 'text': txt_msg})

    # Import Fixations
    print('Parsing fixations...')
    i_not_efix = np.nonzero(lineType != 'EFIX')[0]
    df_fix = pd.read_csv(ascii_file_path, skiprows=i_not_efix, header=None, sep='\s+', usecols=range(1, 8),
                         low_memory=False)
    df_fix.columns = ['eye', 'tStart', 'tEnd', 'duration', 'xAvg', 'yAvg', 'pupilAvg']

    # Saccades
    print('Parsing saccades...')
    i_not_esacc = np.nonzero(lineType != 'ESACC')[0]
    df_sacc = pd.read_csv(ascii_file_path, skiprows=i_not_esacc, header=None, sep='\s+', usecols=range(1, 11),
                          low_memory=False,dtype = {5: str, 6: str, 7: str, 8: str})
    df_sacc.columns = ['eye', 'tStart', 'tEnd', 'duration', 'xStart', 'yStart', 'xEnd', 'yEnd', 'ampDeg', 'vPeak']
    
    # Remove rows which have a '.' in 'xStart', 'yStart', 'xEnd' or 'yEnd' because those are saccades that were not fully detected
    df_sacc = df_sacc[~df_sacc['xStart'].str.contains('.')]
    df_sacc = df_sacc[~df_sacc['yStart'].str.contains('.')]
    df_sacc = df_sacc[~df_sacc['xEnd'].str.contains('.')]
    df_sacc = df_sacc[~df_sacc['yEnd'].str.contains('.')]

    df_sacc['xStart'] = pd.to_numeric(df_sacc['xStart'], errors='raise')
    df_sacc['yStart'] = pd.to_numeric(df_sacc['yStart'], errors='raise')
    df_sacc['xEnd'] = pd.to_numeric(df_sacc['xEnd'], errors='raise')
    df_sacc['yEnd'] = pd.to_numeric(df_sacc['yEnd'], errors='raise')

    # Blinks
    print('Parsing blinks...')
    df_blink = pd.DataFrame()
    i_not_eblink = np.nonzero(lineType != 'EBLINK')[0]
    if len(i_not_eblink) < nLines:
        df_blink = pd.read_csv(ascii_file_path, skiprows=i_not_eblink, header=None, sep='\s+', usecols=range(1, 5),
                               low_memory=False)
        df_blink.columns = ['eye', 'tStart', 'tEnd', 'duration']

    # determine sample columns based on eyes recorded in file
    eyes_in_file = np.unique(df_fix.eye)
    if eyes_in_file.size == 2:
        cols = ['tSample', 'LX', 'LY', 'LPupil', 'RX', 'RY', 'RPupil']
    else:
        eye = eyes_in_file[0]
        print('monocular data detected (%c eye).' % eye)
        cols = ['tSample', '%cX' % eye, '%cY' % eye, '%cPupil' % eye]

    # Import samples
    i_not_sample = np.nonzero(lineType != 'SAMPLE')[0]
    dfSamples = pd.read_csv(ascii_file_path, skiprows=i_not_sample, header=None, sep='\s+',
                                usecols=range(0, len(cols)), low_memory=False)
    dfSamples.columns = cols
    # Convert values to numbers
    for eye in ['L', 'R']:
        if eye in eyes_in_file:
            dfSamples['%cX' % eye] = pd.to_numeric(dfSamples['%cX' % eye], errors='coerce')
            dfSamples['%cY' % eye] = pd.to_numeric(dfSamples['%cY' % eye], errors='coerce')
            dfSamples['%cPupil' % eye] = pd.to_numeric(dfSamples['%cPupil' % eye], errors='coerce')
        else:
            dfSamples['%cX' % eye] = np.nan
            dfSamples['%cY' % eye] = np.nan
            dfSamples['%cPupil' % eye] = np.nan

    dict_events = {'fix': df_fix, 'sacc': df_sacc, 'blink': df_blink}
    if not keep_ascii:
        os.remove(ascii_file_path)

    # Save the 5 data structures in HDF5 file each, in the derivatives folder
    dfHeader.to_hdf(os.path.join(derivatives_folder, 'header.hdf5'), key='dfHeader', mode='w')
    dfMsg.to_hdf(os.path.join(derivatives_folder, 'msg.hdf5'), key='dfMsg', mode='w')
    dfCalib.to_hdf(os.path.join(derivatives_folder, 'calib.hdf5'), key='dfCalib', mode='w')
    dfSamples.to_hdf(os.path.join(derivatives_folder, 'samples.hdf5'), key='dfSamples', mode='w')
    for key, value in dict_events.items():
        value.to_hdf(os.path.join(derivatives_folder, key + '.hdf5'), key=key, mode='w')


def compute_derivatives_for_dataset(bids_dataset_folder, msg_keywords):
    """
    Compute derivatives for a dataset.

    Args:
        dataset_folder (str): Path to the folder containing the dataset.
        msg_keywords (list of str): List of strings representing keywords to filter MSG lines. Leave empty if no
            filtering is required.
        derivatives_folder (str): Path to the folder to save the derivatives.

    Returns:
        None
    """
    derivatives_folder = bids_dataset_folder + "_derivatives"
    # Create the derivatives folder
    os.makedirs(derivatives_folder, exist_ok=True)

    # List of folders in bids_dataset_folder
    bids_folders = os.listdir(bids_dataset_folder)
    # Filter out non-subject folders
    bids_folders = [folder for folder in bids_folders if folder.startswith("sub-")]

    # Compute derivatives for each EDF file in the dataset
    for subject in bids_folders:
        # List of folders in subject folder
        sessions_folders = os.listdir(os.path.join(bids_dataset_folder, subject))
        # Filter out non-session folders
        sessions_folders = [folder for folder in sessions_folders if folder.startswith("ses-")]
        for session in os.listdir(os.path.join(bids_dataset_folder, subject)):
            eye_tracking_data_path = os.path.join(bids_dataset_folder, subject, session, 'ET')
            for file in os.listdir(eye_tracking_data_path):
                if file.lower().endswith(".edf"):
                    edf_file_path = os.path.join(eye_tracking_data_path, file)
                    derivatives_folder_path = os.path.join(derivatives_folder, subject, session)
                    os.makedirs(derivatives_folder_path, exist_ok=True)
                    parse_edf_eyelink(edf_file_path, msg_keywords, derivatives_folder_path)