import os
import shutil
import subprocess
import numpy as np
import pandas as pd

from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
import logging


def dataset_to_bids(target_folder_path, files_folder_path, dataset_name, session_substrings=2):
    """
    Convert a dataset to BIDS format.

    Args:
        target_folder_path (str): Path to the folder where the BIDS dataset will be created.
        files_folder_path (str): Path to the folder containing the EDF files.
        The EDF files are assumed to have the ID of the subject at the beginning of the file name, separated by an underscore.
        dataset_name (str): Name of the BIDS dataset.
        session_substrings (int): Number of substrings to use for the session ID. Default is 2.

    Returns:
        None
    """

    # List all file paths in the folder
    file_paths = []
    for root, dirs, files in os.walk(files_folder_path):
        for file in files:
            file_paths.append(os.path.join(root, file))
    
    file_paths = [file for file in file_paths if file.lower().endswith(".edf") or file.lower().endswith(".bdf") or file.lower().endswith(".log") or file.lower().endswith(".csv")]

    bids_folder_path = os.path.join(target_folder_path, dataset_name)

    subj_ids = list(set([os.path.basename(file).split("_")[0] for file in file_paths if file.lower().endswith(".edf") or file.lower().endswith(".bdf")]))

    # If all of the subjects have numerical IDs, sort them numerically, else sort them alphabetically
    if all(subject_id.isdigit() for subject_id in subj_ids):
        subj_ids.sort(key=int)
    else:
        subj_ids.sort()
    new_subj_ids = [str(subject_index).zfill(4) for subject_index in range(len(subj_ids))]

    # Create subfolders for each session for each subject
    for subject_id in new_subj_ids:
        old_subject_id = subj_ids[int(subject_id)]
        for file in file_paths:
            file_name = os.path.basename(file)
            file_lower = file_name.lower()
            session_id = "_".join(file_name.split("_")[1:session_substrings+1])
            if file_lower.endswith(".edf") and file_name.startswith(old_subject_id):
                move_file_to_bids_folder(file, bids_folder_path, subject_id, session_id, 'ET')
            if file_lower.endswith(".bdf") and file_name.startswith(old_subject_id):
                move_file_to_bids_folder(file, bids_folder_path, subject_id, session_id, 'EEG')
            if (file_lower.endswith(".log") or file_lower.endswith(".csv")) and file_name.startswith(old_subject_id):                
                move_file_to_bids_folder(file, bids_folder_path, subject_id, session_id, 'behavioral')
    return bids_folder_path

def move_file_to_bids_folder(file_path, bids_folder_path, subject_id, session_id,tag):
    session_folder_path = os.path.join(bids_folder_path, "sub-" + subject_id, "ses-" + session_id,tag)
    os.makedirs(session_folder_path, exist_ok=True)
    new_file_path = os.path.join(session_folder_path, os.path.basename(file_path))
    if not os.path.exists(new_file_path):
        shutil.copy(file_path, session_folder_path)
    

def convert_edf_to_ascii(edf_file_path, output_dir):
    """
    Convert an EDF file to ASCII format using edf2asc.

    Args:
        edf_file_path (str): Path to the input EDF file.
        output_dir (str): Directory to save the ASCII file. If None, the ASCII file will be saved in the same directory as the input EDF file.

    Returns:
        str: Path to the generated ASCII file.
    """
    # Check if edf2asc is installed
    if not shutil.which("edf2asc"):
        raise FileNotFoundError("edf2asc not found. Please make sure EyeLink software is installed and accessible in the system PATH.")

    # Set output directory
    if output_dir is None:
        raise ValueError("Output directory must be specified.")

    # Generate output file path
    edf_file_name = os.path.basename(edf_file_path)
    ascii_file_name = os.path.splitext(edf_file_name)[0] + ".asc"
    ascii_file_path = os.path.join(output_dir, ascii_file_name)

    # Run edf2asc command with the -failsafe flag, only run it if the file does not already exist
    if not os.path.exists(ascii_file_path):
        subprocess.run(["edf2asc", "-failsafe", edf_file_path, ascii_file_path])

    return ascii_file_path



def parse_edf_eyelink(edf_file_path, msg_keywords,session_folder_path,keep_ascii=True):
    """
    Parse an EDF file generated by EyeLink system.
    Adapted from `ParseEyeLinkAsc` by DJ. (https://github.com/djangraw/ParseEyeLinkAscFiles)

    Args:
        edf_file_path (str): Path to the input EDF file.
        msg_keywords (list of str): List of strings representing keywords to filter MSG lines.

    Returns:
        tuple: A tuple containing five pandas DataFrames:
            - Header information DataFrame
            - MSG lines DataFrame filtered by msg_keywords
            - Calibration information DataFrame
            - EyeLink events DataFrame
            - Raw sample data DataFrame
    """
    # Configure logging
    derivatives_folder_path = os.path.dirname(os.path.dirname(session_folder_path))
    logging.basicConfig(filename=os.path.join(derivatives_folder_path,'derivatives_generation.log'),level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s') 
    # Convert EDF to ASCII
    ascii_file_path = convert_edf_to_ascii(edf_file_path,session_folder_path)

    #Check if files already exist:
    if os.path.exists(os.path.join(session_folder_path, 'header.hdf5')) and os.path.exists(os.path.join(session_folder_path, 'msg.hdf5')) and os.path.exists(os.path.join(session_folder_path, 'calib.hdf5')) and os.path.exists(os.path.join(session_folder_path, 'samples.hdf5')):
        return

    # ===== READ IN FILES ===== #
    # Read in EyeLink file
    
    f = open(ascii_file_path,'r')
    fileTxt0 = f.read().splitlines(True) # split into lines
    fileTxt0 = np.array(fileTxt0) # concert to np array for simpler indexing
    f.close()

    subject = os.path.basename(edf_file_path).split("_")[0]
    # Separate lines into samples and messages
    logging.info(f'Sorting lines for subject {subject}...')
    nLines = len(fileTxt0)
    lineType = np.array(['OTHER']*nLines,dtype='object')


    # Usar lo de mne, particularmente para calibration.
    # En sample tendría que filtrar lo que viene después de START y antes de END.
    

    calibration_flag = False
    start_flag = False
    for iLine in range(nLines):
        if len(fileTxt0[iLine])<2:
            lineType[iLine] = 'EMPTY'
        elif fileTxt0[iLine].startswith('*'):
            lineType[iLine] = 'HEADER'
        # If there is a !CAL in the line, it is a calibration line
        elif '!CAL' in fileTxt0[iLine]:
            lineType[iLine] = 'Calibration'
            calibration_flag = True
        elif fileTxt0[iLine].split()[0] == 'START' and calibration_flag:
            calibration_flag = False
            start_flag = True
        elif calibration_flag:
            lineType[iLine] = 'Calibration'
        elif not start_flag: # Data before the first successful calibration is discarded. 
            # After the first successul calibration, EVERY sample is taken into account.
            lineType[iLine] = 'Non_calibrated_samples'
        elif fileTxt0[iLine].split()[0] == 'MSG' and any(keyword in fileTxt0[iLine] for keyword in msg_keywords):
            lineType[iLine] = 'MSG'
        elif fileTxt0[iLine].split()[0] == 'ESACC':
            lineType[iLine] = 'ESACC'
        elif fileTxt0[iLine].split()[0] == 'EFIX':
            lineType[iLine] = 'EFIX'
        elif fileTxt0[iLine].split()[0] == 'EBLINK':
            lineType[iLine] = 'EBLINK'
        elif fileTxt0[iLine].split()[0][0].isdigit() or fileTxt0[iLine].split()[0].startswith('-'):
            lineType[iLine] = 'SAMPLE'
        else:
            lineType[iLine] = 'OTHER'
        
 
    # ===== PARSE EYELINK FILE ===== #

    # Import Header
    logging.info(f'Parsing header for subject {subject}...')
    dfHeader = pd.read_csv(ascii_file_path,skiprows=np.nonzero(lineType!='HEADER')[0],header=None,sep='\s+')
    # Merge columns into single strings
    dfHeader = dfHeader.apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1)
    # Change into a DataFrame
    dfHeader = dfHeader.to_frame(name='value')
    # Add line number
    dfHeader["Line_number"] = np.nonzero(lineType=='HEADER')[0]


    # Import Calibration
    logging.info(f'Parsing calibration for subject {subject}...')
    iCal = np.nonzero(lineType!='Calibration')[0]
    dfCalib = pd.read_csv(ascii_file_path,skiprows=iCal,names=np.arange(9))
    # Merge columns into single strings
    dfCalib = dfCalib.apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1)
    # Change into a DataFrame
    dfCalib = dfCalib.to_frame(name='value')
    # Add line number
    dfCalib["Line_number"] = np.nonzero(lineType=='Calibration')[0]
    # Get the first column (which is of type string) and see which row contains the string 'GAZE_COORDS'
    i_gaze_coords = dfCalib['value'].str.contains('GAZE_COORDS')
    # Get the first row that matches the condition
    i_gaze_coords = i_gaze_coords[i_gaze_coords].index[0]
    # Grab the value of that row in the first column, turn it into a string and split it by spaces, then get the 5th and 6th elements, which are the screen resolution
    screen_res = dfCalib.iloc[i_gaze_coords]['value'].split()[5:7]
    # Parse them as integers and then back to strings
    screen_res = [str(int(float(res))) for res in screen_res]
    dfHeader.loc[len(dfHeader.index),'value'] = "** SCREEN SIZE: "+ " ".join(screen_res)



    # Import Message
    logging.info(f'Parsing messages for subject {subject}...')
    i_msg = np.nonzero(lineType == 'MSG')[0]
    t_msg = []
    txt_msg = []
    for i in range(len(i_msg)):
        # separate MSG prefix and timestamp from rest of message
        info = fileTxt0[i_msg[i]].split()
        # extract info
        t_msg.append(int(info[1]))
        txt_msg.append(' '.join(info[2:]))
    dfMsg = pd.DataFrame({'time': t_msg, 'text': txt_msg})
    dfMsg["Line_number"] = i_msg

    # Import Fixations
    logging.info(f'Parsing fixations for subject {subject}...')
    i_not_efix = np.nonzero(lineType != 'EFIX')[0]
    df_fix = pd.read_csv(ascii_file_path, skiprows=i_not_efix, header=None, sep='\s+', usecols=range(1, 8),
                         low_memory=False)
    df_fix.columns = ['eye', 'tStart', 'tEnd', 'duration', 'xAvg', 'yAvg', 'pupilAvg']
    df_fix["Line_number"] = np.nonzero(lineType=='EFIX')[0]

    # Saccades
    logging.info(f'Parsing saccades for subject {subject}...')
    i_not_esacc = np.nonzero(lineType != 'ESACC')[0]
    df_sacc = pd.read_csv(ascii_file_path, skiprows=i_not_esacc, header=None, sep='\s+', usecols=range(1, 11),
                          low_memory=False,dtype = {5: str, 6: str, 7: str, 8: str})
    df_sacc.columns = ['eye', 'tStart', 'tEnd', 'duration', 'xStart', 'yStart', 'xEnd', 'yEnd', 'ampDeg', 'vPeak']
    df_sacc["Line_number"] = np.nonzero(lineType=='ESACC')[0]
    # Remove rows which don't have a digit in the xStart, yStart, xEnd, yEnd columns
    filtering = df_sacc['xStart'].apply(lambda x: not any(char.isdigit() for char in x)) | df_sacc['yStart'].apply(lambda x: not any(char.isdigit() for char in x)) | df_sacc['xEnd'].apply(lambda x: not any(char.isdigit() for char in x)) | df_sacc['yEnd'].apply(lambda x: not any(char.isdigit() for char in x))
    df_sacc = df_sacc[~filtering]

    df_sacc['xStart'] = pd.to_numeric(df_sacc['xStart'], errors='raise')
    df_sacc['yStart'] = pd.to_numeric(df_sacc['yStart'], errors='raise')
    df_sacc['xEnd'] = pd.to_numeric(df_sacc['xEnd'], errors='raise')
    df_sacc['yEnd'] = pd.to_numeric(df_sacc['yEnd'], errors='raise')



    # Blinks
    logging.info(f'Parsing blinks for subject {subject}...')
    df_blink = pd.DataFrame()
    i_not_eblink = np.nonzero(lineType != 'EBLINK')[0]
    if len(i_not_eblink) < nLines:
        df_blink = pd.read_csv(ascii_file_path, skiprows=i_not_eblink, header=None, sep='\s+', usecols=range(1, 5),
                               low_memory=False)
        df_blink.columns = ['eye', 'tStart', 'tEnd', 'duration']
    df_blink["Line_number"] = np.nonzero(lineType=='EBLINK')[0]

    # determine sample columns based on eyes recorded in file
    eyes_in_file = np.unique(df_fix.eye)
    if eyes_in_file.size == 2:
        cols = ['tSample', 'LX', 'LY', 'LPupil', 'RX', 'RY', 'RPupil']
    else:
        eye = eyes_in_file[0]
        logging.info(f'monocular data ({eye} eye) recording for subject {subject}...')
        cols = ['tSample', '%cX' % eye, '%cY' % eye, '%cPupil' % eye]

    # Import samples
    i_not_sample = np.nonzero(lineType != 'SAMPLE')[0]
    dfSamples = pd.read_csv(ascii_file_path, skiprows=i_not_sample, header=None, sep='\s+',
                                usecols=range(0, len(cols)), low_memory=False)
    dfSamples.columns = cols
    # Convert values to numbers
    for eye in ['L', 'R']:
        if eye in eyes_in_file:
            dfSamples['%cX' % eye] = pd.to_numeric(dfSamples['%cX' % eye], errors='coerce')
            dfSamples['%cY' % eye] = pd.to_numeric(dfSamples['%cY' % eye], errors='coerce')
            dfSamples['%cPupil' % eye] = pd.to_numeric(dfSamples['%cPupil' % eye], errors='coerce')


    dfSamples["Line_number"] = np.nonzero(lineType=='SAMPLE')[0]
    dict_events = {'fix': df_fix, 'sacc': df_sacc, 'blink': df_blink}
    if not keep_ascii:
        os.remove(ascii_file_path)

    # Save the 5 data structures in HDF5 file each, in the derivatives folder
    dfHeader.to_hdf(os.path.join(session_folder_path, 'header.hdf5'), key='header', mode='w')
    dfMsg.to_hdf(os.path.join(session_folder_path, 'msg.hdf5'), key='msg', mode='w')
    dfCalib.to_hdf(os.path.join(session_folder_path, 'calib.hdf5'), key='calib', mode='w')
    dfSamples.to_hdf(os.path.join(session_folder_path, 'samples.hdf5'), key='samples', mode='w')
    for key, value in dict_events.items():
        value.to_hdf(os.path.join(session_folder_path,"eyelink_events", key + '.hdf5'), key=key, mode='w')


def process_edf_file(edf_file_path, msg_keywords, session_folder_path):
    parse_edf_eyelink(edf_file_path, msg_keywords, session_folder_path)

def process_session(bids_dataset_folder, subject, session, msg_keywords, derivatives_folder,num_threads):
    eye_tracking_data_path = os.path.join(bids_dataset_folder, subject, session, 'ET')
    edf_files = [file for file in os.listdir(eye_tracking_data_path) if file.lower().endswith(".edf")]
    if len(edf_files) > 1:
        logging.warning(f"More than one EDF file found in {eye_tracking_data_path}. Skipping folder.")
        return
    
    with ThreadPoolExecutor(max_workers=num_threads) as executor:
        futures = []
        for file in edf_files:
            edf_file_path = os.path.join(eye_tracking_data_path, file)
            session_folder_path = os.path.join(derivatives_folder, subject, session)
            os.makedirs(os.path.join(session_folder_path, 'eyelink_events'), exist_ok=True)
            futures.append(executor.submit(process_edf_file, edf_file_path, msg_keywords, session_folder_path))
        for future in futures:
            future.result() # This will raise exceptions if any occurred during processing

def process_subject(bids_dataset_folder, subject, msg_keywords, derivatives_folder,num_threads):
    sessions_folders = [folder for folder in os.listdir(os.path.join(bids_dataset_folder, subject)) if folder.startswith("ses-")]
    for session in sessions_folders:
        process_session(bids_dataset_folder, subject, session, msg_keywords, derivatives_folder,num_threads)

def compute_derivatives_for_dataset(bids_dataset_folder, msg_keywords,num_processes=None,num_threads=None):
    '''
    Generate the derivatives for a BIDS dataset.

    Args:
        bids_dataset_folder (str): Path to the BIDS dataset folder.
        msg_keywords (list of str): List of strings representing keywords to filter MSG lines. For example: 'trial_start', 'trial_end'.
    '''
    derivatives_folder = bids_dataset_folder + "_derivatives"
    os.makedirs(derivatives_folder, exist_ok=True)

    bids_folders = [folder for folder in os.listdir(bids_dataset_folder) if folder.startswith("sub-")]

    with ProcessPoolExecutor(max_workers=num_processes) as executor:
        futures = []
        for subject in bids_folders:
            futures.append(executor.submit(process_subject, bids_dataset_folder, subject, msg_keywords, derivatives_folder,num_threads))
        for futures in futures:
            futures.result()  # This will raise exceptions if any occurred during processing
    return derivatives_folder
        